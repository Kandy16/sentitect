{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD DATA SETS\n",
    "# Load the train, test and submission data frames\n",
    "\n",
    "TOY_NUM = 200\n",
    "\n",
    "print('--------------- Data preparation ------------------')\n",
    "\n",
    "train_df = pd.read_csv(\"train-data.csv\")\n",
    "train_df = shuffle(train_df)[0:TOY_NUM]\n",
    "\n",
    "test_df = pd.read_csv(\"test-data.csv\")\n",
    "test_df = shuffle(test_df)[0:TOY_NUM]\n",
    "\n",
    "submission_df = pd.read_csv(\"predict-data.csv\")\n",
    "submission_df = shuffle(submission_df)[0:TOY_NUM]\n",
    "\n",
    "# Create a merged data set and review initial information\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "# DATA EXPLORATION\n",
    "\n",
    "# Quickly check for class imbalance\n",
    "print(combined_df.describe())\n",
    "\n",
    "# Check what the text looks like\n",
    "print(combined_df.head(5))\n",
    "\n",
    "# Get all the unique keywords\n",
    "#print(combined_df[\"review\"]-str.split.unique())\n",
    "\n",
    "# Create small function to clean text\n",
    "def text_clean(text):\n",
    "\n",
    "    for element in [\"http\\S+\", \"RT \", \"[^a-zA-Z\\'\\.\\,\\d\\s]\", \"[0-9]\",\"\\t\", \"\\n\", \"\\s+\", \"<.*?>\"]:\n",
    "        text = re.sub(\"r\"+element, \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean data sets\n",
    "combined_df.review = combined_df.review.apply(text_clean)\n",
    "#test_df.review = test_df.review.apply(text_clean)\n",
    "submission_df.review = submission_df.review.apply(text_clean)\n",
    "\n",
    "# CORRECT SPELLING\n",
    "\n",
    "# Instantiate spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Correct spelling\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "# Spellcheck data sets\n",
    "#train_df.text = train_df.text.apply(correct_spellings)\n",
    "#val_df.text = val_df.text.apply(correct_spellings)\n",
    "\n",
    "print('--------------- Vectorizing data ------------------')\n",
    "\n",
    "start_vector_time = time.time()\n",
    "\n",
    "#VECTORIZE the sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_sm', \n",
    "                exclude=['tagger','parser','ner','entity_linker','entity_ruler',\n",
    "                'textcat','textcat_multilabel','lemmatizer', 'morphologizer',\n",
    "                'attribute_ruler','senter','sentencizer','tok2vec','transformer'])\n",
    "'''\n",
    "\n",
    "\n",
    "# Embed sentences for the training set\n",
    "X_train = []\n",
    "for r in nlp.pipe(combined_df.review.values, disable=['parser','ner','entity_linker','entity_ruler',\n",
    "                'textcat','textcat_multilabel','lemmatizer', 'morphologizer',\n",
    "                'attribute_ruler','senter','sentencizer','tok2vec','transformer']):\n",
    "\n",
    "    #print(f\"{idx} out of {train_df.shape[0]}\")\n",
    "    emb = r.vector\n",
    "    review_emb = emb.reshape(-1)\n",
    "    X_train.append(review_emb)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = combined_df.sentiment.values\n",
    "\n",
    "end_vector_time = time.time()\n",
    "\n",
    "print(f'Vectorization Time taken in seconds : {end_vector_time - start_vector_time}')\n",
    "print(f'Vectorization Time taken in minutes : {(end_vector_time - start_vector_time)/60}')\n",
    "\n",
    "'''\n",
    "\n",
    "# Embed sentences for the submission set\n",
    "submission_data = []\n",
    "for r in nlp.pipe(submission_df.review.values):\n",
    "    emb = r.vector\n",
    "    review_emb = emb.reshape(-1)\n",
    "    submission_data.append(review_emb)\n",
    "\n",
    "submission_data = np.array(submission_data)\n",
    "\n",
    "'''\n",
    "print('--------------- Training Data ------------------')\n",
    "\n",
    "\n",
    "# LGBM\n",
    "\n",
    "# Split data into train and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2, random_state = 42)\n",
    "\n",
    "# Get the train and test data for the training sequence\n",
    "train_data = lgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lgbm.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Parameters we'll use for the prediction\n",
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting': 'dart',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the classifier\n",
    "classifier = lgbm.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets= test_data,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)\n",
    "\n",
    "'''\n",
    "\n",
    "print('--------------- Prediction ------------------')\n",
    "\n",
    "# PREDICTION\n",
    "val_pred = classifier.predict(submission_data)\n",
    "\n",
    "# Submission file\n",
    "submission_df['sentiment_predicted'] = val_pred.round().astype(int)\n",
    "submission_df.to_csv('submission_lgbm.csv', index=False)\n",
    "\n",
    "'''\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken in seconds : {end_time - start_time}')\n",
    "print(f'Time taken in minutes : {(end_time - start_time)/60}')\n",
    "\n",
    "\n",
    "#correct_pred_count =  sum(submission_df['sentiment'] == submission_df['sentiment_predicted'])\n",
    "#print('The accuracy is : ', (100*correct_pred_count/submission_df.shape[0]))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgbm\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD DATA SETS\n",
    "# Load the train, test and submission data frames\n",
    "\n",
    "TOY_NUM = 200\n",
    "\n",
    "print('--------------- Data preparation ------------------')\n",
    "\n",
    "train_df = pd.read_csv(\"train-data.csv\")\n",
    "train_df = shuffle(train_df)[0:TOY_NUM]\n",
    "\n",
    "test_df = pd.read_csv(\"test-data.csv\")\n",
    "test_df = shuffle(test_df)[0:TOY_NUM]\n",
    "\n",
    "submission_df = pd.read_csv(\"predict-data.csv\")\n",
    "submission_df = shuffle(submission_df)[0:TOY_NUM]\n",
    "\n",
    "# Create a merged data set and review initial information\n",
    "combined_df = pd.concat([train_df, test_df])\n",
    "\n",
    "# DATA EXPLORATION\n",
    "\n",
    "# Quickly check for class imbalance\n",
    "print(combined_df.describe())\n",
    "\n",
    "# Check what the text looks like\n",
    "print(combined_df.head(5))\n",
    "\n",
    "print('--------------- Vectorizing data ------------------')\n",
    "\n",
    "start_vector_time = time.time()\n",
    "\n",
    "#VECTORIZE the sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Embed sentences for the training set\n",
    "X_train = []\n",
    "for r in nlp.pipe(combined_df.review.values, disable=['parser','ner','entity_linker','entity_ruler',\n",
    "                'textcat','textcat_multilabel','lemmatizer', 'morphologizer',\n",
    "                'attribute_ruler','senter','sentencizer','tok2vec','transformer']):\n",
    "\n",
    "    #print(f\"{idx} out of {train_df.shape[0]}\")\n",
    "    emb = r.vector\n",
    "    review_emb = emb.reshape(-1)\n",
    "    X_train.append(review_emb)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = combined_df.sentiment.values\n",
    "\n",
    "end_vector_time = time.time()\n",
    "\n",
    "print(f'Vectorization Time taken in seconds : {end_vector_time - start_vector_time}')\n",
    "print(f'Vectorization Time taken in minutes : {(end_vector_time - start_vector_time)/60}')\n",
    "\n",
    "# LGBM\n",
    "\n",
    "# Split data into train and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2, random_state = 42)\n",
    "\n",
    "# Get the train and test data for the training sequence\n",
    "train_data = lgbm.Dataset(X_train, label=y_train)\n",
    "test_data = lgbm.Dataset(X_test, label=y_test)\n",
    "\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [(X_test,y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "\t\t\t 'boosting_type':['gbdt','goss'],\n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(X_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "\n",
    "clf_sw = lgb.LGBMClassifier(**clf.get_params())\n",
    "#set optimal parameters\n",
    "clf_sw.set_params(**gs.best_estimator_.get_params())\n",
    "gs_sample_weight = GridSearchCV(estimator=clf_sw, \n",
    "                                param_grid={'scale_pos_weight':[1,2,6,12]},\n",
    "                                scoring='roc_auc',\n",
    "                                cv=5,\n",
    "                                refit=True,\n",
    "                                verbose=True)\n",
    "\n",
    "gs_sample_weight.fit(X_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs_sample_weight.best_score_, gs_sample_weight.best_params_))\n",
    "\n",
    "\n",
    "\n",
    "#Configure from the HP optimisation\n",
    "clf_final = lgb.LGBMClassifier(**clf_sw.get_params())\n",
    "res = clf_final.set_params(**gs_sample_weight.best_estimator_.get_params())\n",
    "print(res)\n",
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "\tbase_learning_rate = 0.1\n",
    "\tlr = base_learning_rate  * np.power(.99, current_iter)\n",
    "\treturn lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "\tprint(current_iter)\n",
    "\n",
    "\tbase_learning_rate = 0.1\n",
    "\tlr = base_learning_rate  * np.power(.995, current_iter)\n",
    "\treturn lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "\tbase_learning_rate = 0.05\n",
    "\tlr = base_learning_rate  * np.power(.99, current_iter)\n",
    "\treturn lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "#clf_final.fit(X_train, y_train, **fit_params, \n",
    "#\t\t\tcallbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])\n",
    "clf_final.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "clf_final.booster_.save_model('final-model.txt')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken in seconds : {end_time - start_time}')\n",
    "print(f'Time taken in minutes : {(end_time - start_time)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LOAD DATA SETS\n",
    "# Load the train, test and submission data frames\n",
    "\n",
    "TOY_NUM = 200\n",
    "\n",
    "print('--------------- Data preparation ------------------')\n",
    "\n",
    " \n",
    "submission_df = pd.read_csv(\"predict-data.csv\")\n",
    "submission_df = shuffle(submission_df)[0:TOY_NUM]\n",
    "\n",
    "# DATA EXPLORATION\n",
    "\n",
    "# Quickly check for class imbalance\n",
    "print(submission_df.describe())\n",
    "\n",
    "# Check what the text looks like\n",
    "print(submission_df.head(5))\n",
    "\n",
    "print('--------------- Vectorizing data ------------------')\n",
    "\n",
    "start_vector_time = time.time()\n",
    "\n",
    "#VECTORIZE the sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_sm', \n",
    "                exclude=['tagger','parser','ner','entity_linker','entity_ruler',\n",
    "                'textcat','textcat_multilabel','lemmatizer', 'morphologizer',\n",
    "                'attribute_ruler','senter','sentencizer','tok2vec','transformer'])\n",
    "'''\n",
    "\n",
    "\n",
    "# Embed sentences for the prediction set\n",
    "predict_data = []\n",
    "for r in nlp.pipe(submission_df.review.values, disable=['parser','ner','entity_linker','entity_ruler',\n",
    "                'textcat','textcat_multilabel','lemmatizer', 'morphologizer',\n",
    "                'attribute_ruler','senter','sentencizer','tok2vec','transformer']):\n",
    "\n",
    "    #print(f\"{idx} out of {train_df.shape[0]}\")\n",
    "    emb = r.vector\n",
    "    review_emb = emb.reshape(-1)\n",
    "    predict_data.append(review_emb)\n",
    "\n",
    "predict_data = np.array(predict_data)\n",
    "\n",
    "end_vector_time = time.time()\n",
    "\n",
    "print(f'Vectorization Time taken in seconds : {end_vector_time - start_vector_time}')\n",
    "print(f'Vectorization Time taken in minutes : {(end_vector_time - start_vector_time)/60}')\n",
    "\n",
    "print('--------------- Predicting Data ------------------')\n",
    "\n",
    "bst = lgbm.Booster(model_file='final-model.txt')\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "val_pred = bst.predict(predict_data)\n",
    "\n",
    "# Submission file\n",
    "submission_df['sentiment_predicted'] = val_pred.round().astype(int)\n",
    "submission_df.to_csv('submission_lgbm.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken in seconds : {end_time - start_time}')\n",
    "print(f'Time taken in minutes : {(end_time - start_time)/60}')\n",
    "\n",
    "#correct_pred_count =  sum(submission_df['sentiment'] == submission_df['sentiment_predicted'])\n",
    "#print('The accuracy is : ', (100*correct_pred_count/submission_df.shape[0]))"
   ]
  }
 ]
}